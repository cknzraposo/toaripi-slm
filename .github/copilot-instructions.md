# GitHub Copilot Instructions â€” Toaripi SLM (Educational Content Generator)
**Note:** This file provides context and instructions for GitHub Copilot to assist with code generation in this repository. It is not part of the user-facing documentation.
## ğŸ§­ Project Context (what Copilot should know)

- **Mission:** Build a **small language model (SLM)** for **Toaripi (ISO 639â€‘3: `tqo`)** to generate **original educational content** (stories, vocabulary, Q&A, dialogues) for primary learners and teachers.
- **Approach:** Fineâ€‘tune a compact open model (â‰ˆ1â€“7B params) on **aligned Englishâ†”Toaripi Bible** data; run **online and offline** (Raspberryâ€¯Pi / CPUâ€‘only) using **quantization** and **llama.cpp**.
- **Users:** Teachers, Toaripi speakers, contributors (linguists, devs) with varying technical experience.
- **Nonâ€‘goals:** Theology tooling / doctrinal outputs; general-purpose chatbot. Focus is **education & language preservation**.

---

## ğŸ§° Tech Stack & Key Tools

- **Language:** Python 3.10+
- **Core libs:** `transformers`, `datasets`, `accelerate`, `peft` (LoRA), `sentencepiece`/tokenizers
- **Serving/UI:** `fastapi` + `uvicorn` (or `streamlit` for quick demo)
- **Edge inference:** `llama.cpp` (GGUF quantized weights)
- **Data formats:** CSV/TSV for parallel verses; optional USFM ingestion
- **Config:** YAML/TOML for data sources & training params

---

## ğŸ“ Repository Shape (expected files and roles)