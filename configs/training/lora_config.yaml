# LoRA fine-tuning configuration for efficient training
model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
  cache_dir: "./models/cache"
  trust_remote_code: false
  device_map: "auto"

lora:
  enabled: true
  rank: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  # Training hyperparameters optimized for LoRA
  epochs: 2
  learning_rate: 1e-4
  batch_size: 1  # Small batch size for memory efficiency
  gradient_accumulation_steps: 16
  warmup_ratio: 0.1
  weight_decay: 0.001
  
  # Evaluation and saving
  eval_strategy: "steps"
  eval_steps: 250
  save_strategy: "steps"
  save_steps: 250
  logging_steps: 50
  
  # Memory optimization
  gradient_checkpointing: true
  dataloader_pin_memory: false

data:
  max_length: 1024
  padding: true
  truncation: true
  return_tensors: "pt"

optimization:
  optimizer: "adamw"
  lr_scheduler_type: "cosine"
  fp16: true
  bf16: false  # Use bf16 if supported by hardware
  tf32: true   # Enable for A100/H100 GPUs

output:
  checkpoint_dir: "./checkpoints"
  save_total_limit: 2
  push_to_hub: false
  hub_model_id: ""

logging:
  use_wandb: true
  project_name: "toaripi-slm"
  run_name: "mistral-7b-lora"
  log_level: "INFO"
  
memory:
  max_memory_mb: 24000  # Adjust based on available GPU memory
  cpu_offload: true