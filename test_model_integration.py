#!/usr/bin/env python3
"""
Test script for the model-integrated Toaripi SLM Interactive CLI.
Tests both demo mode and real model integration.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from src.toaripi_slm.cli.commands.interact import BilingualDisplay, ToaripiGenerator

def test_model_integration():
    """Test the model integration capabilities."""
    console = Console()
    
    # Title
    title_text = Text()
    title_text.append("ü§ñ Toaripi SLM Model Integration Test", style="bold blue")
    
    console.print("\n")
    console.print(Panel(title_text, border_style="blue"))
    
    console.print("\nüìã [bold cyan]Testing Model Integration[/bold cyan]\n")
    
    # Test 1: Demo mode (no real model available)
    console.print("üé≠ [green]Test 1: Demo Mode (No Trained Model)[/green]")
    console.print("Testing fallback behavior when no trained model is available\n")
    
    display = BilingualDisplay(console)
    generator = ToaripiGenerator(Path("./models/nonexistent"))
    generator.load_model()
    display.set_generator(generator)
    
    console.print("üìù [yellow]Chat Question:[/yellow] What is a dog?")
    english_response, toaripi_response = generator.generate_chat_response("What is a dog?")
    display.display_bilingual_content(english_response, toaripi_response, "chat")
    
    console.print("\nüìù [yellow]Story Generation:[/yellow] Tell a story about fishing")
    english_content, toaripi_content = generator.generate_bilingual_content("Tell a story about fishing", "story")
    display.display_bilingual_content(english_content, toaripi_content, "story")
    
    # Test 2: Check for real model availability
    console.print("\nüîç [green]Test 2: Checking for Trained Models[/green]")
    
    possible_model_paths = [
        Path("./models/hf"),
        Path("./models/toaripi-slm"),
        Path("./models/fine-tuned"),
        Path("../models"),
        Path("./training_sessions")
    ]
    
    found_models = []
    for path in possible_model_paths:
        if path.exists():
            config_file = path / "config.json"
            if config_file.exists():
                found_models.append(path)
                console.print(f"‚úÖ Found potential model at: [cyan]{path}[/cyan]")
            else:
                console.print(f"üìÇ Directory exists but no config.json: [dim]{path}[/dim]")
        else:
            console.print(f"‚ùå Path does not exist: [dim]{path}[/dim]")
    
    if found_models:
        console.print(f"\nüéØ [green]Test 3: Attempting to Load Real Model[/green]")
        
        # Try to load the first found model
        model_path = found_models[0]
        console.print(f"Loading model from: [cyan]{model_path}[/cyan]")
        
        real_generator = ToaripiGenerator(model_path)
        success = real_generator.load_model()
        
        if success and real_generator.model is not None:
            console.print("‚úÖ [green]Real model loaded successfully![/green]")
            display.set_generator(real_generator)
            
            console.print("\nüìù [yellow]Testing Real Model Chat:[/yellow] What is water?")
            english_response, toaripi_response = real_generator.generate_chat_response("What is water?")
            display.display_bilingual_content(english_response, toaripi_response, "chat")
            
        else:
            console.print("‚ö†Ô∏è  [yellow]Model loading failed, using demo mode[/yellow]")
    
    else:
        console.print("\nüí° [yellow]No trained models found. To test with a real model:[/yellow]")
        console.print("   1. Train a model using: [cyan]toaripi train[/cyan]")
        console.print("   2. Or place a trained model in: [cyan]./models/hf/[/cyan]")
        console.print("   3. Ensure the model directory contains: [cyan]config.json[/cyan], [cyan]pytorch_model.bin[/cyan], [cyan]tokenizer.json[/cyan]")
    
    # Test 3: Token weight extraction differences
    console.print("\nüé® [green]Test 4: Token Weight Extraction Methods[/green]")
    
    sample_text = "hanere/fish - Swimming creatures caught in rivers for food"
    
    console.print("üìä [yellow]Simulated vs Model-based token weights:[/yellow]")
    console.print(f"Text: {sample_text}")
    
    # Show simulated weights
    simulated_weights = generator._simulate_token_weights(sample_text)
    console.print("\nüé≠ [cyan]Simulated weights:[/cyan]")
    for tw in simulated_weights:
        color = tw.get_color_style()
        console.print(f"  {tw.token}: {tw.weight:.2f}", style=color)
    
    # Show model-based weights (if available)
    if 'real_generator' in locals() and real_generator.model is not None:
        model_weights = real_generator.extract_token_weights_from_model("", sample_text)
        console.print("\nü§ñ [cyan]Model-based weights:[/cyan]")
        for tw in model_weights:
            color = tw.get_color_style()
            console.print(f"  {tw.token}: {tw.weight:.2f}", style=color)
    else:
        console.print("\nü§ñ [dim]Model-based weights: Not available (no trained model loaded)[/dim]")
    
    # Show integration benefits
    console.print("\nüí° [green]Integration Benefits[/green]")
    
    benefits_panel = Panel(
        """
[bold yellow]Demo Mode (No Trained Model):[/bold yellow]
‚Ä¢ Uses static fallback responses for demonstration
‚Ä¢ Simulated token weights based on linguistic heuristics
‚Ä¢ Shows interface and visualization capabilities
‚Ä¢ Provides examples of expected format

[bold yellow]Real Model Mode (Trained SLM Loaded):[/bold yellow]
‚Ä¢ Generates actual Toaripi responses from trained model
‚Ä¢ Extracts real attention weights from model layers
‚Ä¢ Responds to user prompts with learned knowledge
‚Ä¢ Provides authentic language generation

[bold yellow]How to Enable Real Model Mode:[/bold yellow]
1. Train a Toaripi SLM using the training pipeline
2. Ensure model files are in ./models/hf/ directory
3. Model must include: config.json, pytorch_model.bin, tokenizer files
4. Run 'toaripi interact' - system will auto-detect and load model
        """,
        title="üîß Model Integration Guide",
        border_style="cyan"
    )
    
    console.print(benefits_panel)
    
    console.print("\n‚úÖ [bold green]Model integration test completed![/bold green]")
    console.print("The system seamlessly switches between demo and real model modes.")

def check_dependencies():
    """Check if required dependencies for model loading are available."""
    console = Console()
    
    console.print("\nüîç [bold cyan]Checking Model Dependencies[/bold cyan]\n")
    
    dependencies = [
        ("transformers", "HuggingFace Transformers library"),
        ("torch", "PyTorch deep learning framework"),
        ("accelerate", "HuggingFace Accelerate for model loading"),
        ("peft", "Parameter Efficient Fine-Tuning (LoRA)")
    ]
    
    missing_deps = []
    
    for dep, description in dependencies:
        try:
            __import__(dep)
            console.print(f"‚úÖ {dep}: [green]Available[/green] - {description}")
        except ImportError:
            console.print(f"‚ùå {dep}: [red]Missing[/red] - {description}")
            missing_deps.append(dep)
    
    if missing_deps:
        console.print(f"\nüí° [yellow]Install missing dependencies:[/yellow]")
        console.print(f"   pip install {' '.join(missing_deps)}")
        return False
    else:
        console.print(f"\n‚úÖ [green]All dependencies available for model loading![/green]")
        return True

if __name__ == "__main__":
    try:
        # Check dependencies first
        deps_ok = check_dependencies()
        
        if deps_ok:
            test_model_integration()
        else:
            print("\n‚ö†Ô∏è  Some dependencies missing. Demo mode will work, but real model loading requires all dependencies.")
        
        print("\n" + "="*60)
        print("üéâ Model Integration Test Complete!")
        print("="*60)
        print("\nThe enhanced CLI now supports:")
        print("  ‚Ä¢ Automatic model detection and loading")
        print("  ‚Ä¢ Real SLM generation when model available")
        print("  ‚Ä¢ Demo mode fallback for testing interface")
        print("  ‚Ä¢ Token weight extraction from model attention")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)