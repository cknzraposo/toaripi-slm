"""
Model management commands for the Toaripi SLM CLI.
Handles model operations, GGUF export, and educational content validation.
"""

import json
import click
from pathlib import Path
from typing import List, Optional, Dict, Any
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel

from ..models.enums import ModelSize, DeviceType, ContentType, AgeGroup
from ..utils.helpers import get_file_hash, format_file_size

console = Console()


@click.group()
def model():
    """Model management commands for Toaripi SLM."""
    pass


@model.command()
@click.option('--directory', '-d', type=Path, default=Path("models"),
              help='Directory to scan for models')
@click.option('--format', 'model_format', type=click.Choice(['all', 'hf', 'gguf', 'checkpoint']),
              default='all', help='Filter by model format')
@click.option('--show-details', is_flag=True, help='Show detailed model information')
def list(directory: Path, model_format: str, show_details: bool):
    """List available models and their properties."""
    
    console.print("\nü§ñ [bold blue]Available Toaripi Models[/bold blue]\n")
    
    if not directory.exists():
        console.print(f"‚ùå [red]Models directory not found: {directory}[/red]")
        raise click.Abort()
    
    # Find model files based on format
    model_files = []
    if model_format in ['all', 'gguf']:
        model_files.extend(directory.rglob("*.gguf"))
    if model_format in ['all', 'hf']:
        # Look for HuggingFace models (directories with config.json)
        for config_file in directory.rglob("config.json"):
            model_files.append(config_file.parent)
    if model_format in ['all', 'checkpoint']:
        model_files.extend(directory.rglob("*.bin"))
        model_files.extend(directory.rglob("*.pth"))
        model_files.extend(directory.rglob("*.safetensors"))
    
    if not model_files:
        console.print(f"No models found in {directory}")
        if model_format != 'all':
            console.print(f"Try using --format all to see models in other formats")
        return
    
    # Create models table
    table = Table()
    table.add_column("Model", style="cyan")
    table.add_column("Format", style="yellow")
    table.add_column("Size", justify="right", style="white")
    table.add_column("Location", style="dim white")
    
    if show_details:
        table.add_column("Device", style="green")
        table.add_column("Modified", style="blue")
        table.add_column("Hash", style="magenta")
    
    for model_path in sorted(model_files):
        # Determine model format
        if model_path.is_dir():
            # HuggingFace model directory
            format_str = "HF"
            size = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
            model_name = model_path.name
        else:
            # Single model file
            if model_path.suffix == '.gguf':
                format_str = "GGUF"
            elif model_path.suffix in ['.bin', '.pth']:
                format_str = "PyTorch"
            elif model_path.suffix == '.safetensors':
                format_str = "SafeTensors"
            else:
                format_str = model_path.suffix[1:].upper()
            
            size = model_path.stat().st_size
            model_name = model_path.name
        
        # Calculate relative path
        try:
            relative_path = model_path.relative_to(Path.cwd())
        except ValueError:
            relative_path = model_path
        
        row = [
            model_name,
            format_str,
            format_file_size(size),
            str(relative_path.parent)
        ]
        
        if show_details:
            # Infer device compatibility from model size
            size_mb = size / (1024 * 1024)
            if size_mb < 100:
                device = "All"
            elif size_mb < 1000:
                device = "CPU/GPU"
            elif size_mb < 4000:
                device = "GPU"
            else:
                device = "GPU (High-end)"
            
            # Get modification time
            if model_path.is_dir():
                # Use the most recent file in the directory
                mod_time = max(f.stat().st_mtime for f in model_path.rglob("*") if f.is_file())
            else:
                mod_time = model_path.stat().st_mtime
            
            import datetime
            modified_str = datetime.datetime.fromtimestamp(mod_time).strftime("%Y-%m-%d %H:%M")
            
            # Calculate hash (for files only, not directories)
            if model_path.is_file():
                try:
                    file_hash = get_file_hash(model_path)[:8] + "..."
                except:
                    file_hash = "Error"
            else:
                file_hash = "N/A"
            
            row.extend([device, modified_str, file_hash])
        
        table.add_row(*row)
    
    console.print(table)
    console.print(f"\nFound {len(model_files)} models")


@model.command()
@click.option('--model', '-m', required=True, type=Path, help='Model path to export')
@click.option('--output', '-o', type=Path, help='Output directory for GGUF model')
@click.option('--quantization', '-q', 
              type=click.Choice(['q4_k_m', 'q5_k_m', 'q8_0', 'q4_0', 'q5_0', 'q6_k', 'q8_k']),
              default='q4_k_m', help='Quantization level')
@click.option('--device-target', type=click.Choice([dt.value for dt in DeviceType]),
              default='raspberry_pi', help='Target deployment device')
@click.option('--educational-validation', is_flag=True, default=True,
              help='Validate educational content generation capability')
@click.option('--dry-run', is_flag=True, help='Show what would be done without executing')
def export(model: Path, output: Optional[Path], quantization: str, 
          device_target: str, educational_validation: bool, dry_run: bool):
    """Export model to GGUF format for edge deployment."""
    
    console.print("\nüì¶ [bold blue]Exporting Model to GGUF Format[/bold blue]\n")
    
    # Validate input model
    if not model.exists():
        console.print(f"‚ùå [red]Model not found: {model}[/red]")
        raise click.Abort()
    
    # Set default output path
    if not output:
        output = Path("models/gguf")
    
    output.mkdir(parents=True, exist_ok=True)
    
    # Convert device target to enum
    device_enum = DeviceType(device_target)
    
    if dry_run:
        console.print("üß™ [yellow]DRY RUN MODE - No files will be exported[/yellow]\n")
    
    # Display export configuration
    config_table = Table(title="Export Configuration")
    config_table.add_column("Setting", style="cyan")
    config_table.add_column("Value", style="white")
    
    config_table.add_row("Source Model", str(model))
    config_table.add_row("Output Directory", str(output))
    config_table.add_row("Quantization", quantization.upper())
    config_table.add_row("Target Device", device_target.replace('_', ' ').title())
    config_table.add_row("Educational Validation", "‚úì Enabled" if educational_validation else "‚úó Disabled")
    
    console.print(config_table)
    console.print()
    
    if not dry_run:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            
            # Validate source model
            task1 = progress.add_task("Validating source model...", total=1)
            try:
                model_info = _get_model_info(model)
                progress.update(task1, completed=1)
                console.print(f"‚úì [green]Model validated: {model_info['type']}[/green]")
            except Exception as e:
                progress.update(task1, completed=1)
                console.print(f"‚ùå [red]Model validation failed: {e}[/red]")
                raise click.Abort()
            
            # Generate output filename
            model_name = model.stem if model.is_file() else model.name
            output_file = output / f"{model_name}_{quantization}.gguf"
            
            # Export to GGUF
            task2 = progress.add_task("Converting to GGUF format...", total=1)
            try:
                _export_to_gguf(model, output_file, quantization, device_enum)
                progress.update(task2, completed=1)
                console.print(f"‚úì [green]GGUF export completed: {output_file}[/green]")
            except Exception as e:
                progress.update(task2, completed=1)
                console.print(f"‚ùå [red]GGUF export failed: {e}[/red]")
                raise click.Abort()
            
            # Educational content validation
            if educational_validation:
                task3 = progress.add_task("Validating educational content generation...", total=1)
                try:
                    validation_results = _validate_educational_generation(output_file)
                    progress.update(task3, completed=1)
                    
                    if validation_results["passed"]:
                        console.print(f"‚úì [green]Educational validation passed[/green]")
                    else:
                        console.print(f"‚ö†Ô∏è  [yellow]Educational validation warnings: {validation_results['issues']}[/yellow]")
                except Exception as e:
                    progress.update(task3, completed=1)
                    console.print(f"‚ùå [red]Educational validation failed: {e}[/red]")
        
        # Display export results
        console.print("\nüìä [bold green]Export Results[/bold green]\n")
        
        results_table = Table()
        results_table.add_column("Property", style="cyan")
        results_table.add_column("Value", style="white")
        
        output_size = output_file.stat().st_size if output_file.exists() else 0
        
        results_table.add_row("Source Model", str(model))
        results_table.add_row("Output File", str(output_file))
        results_table.add_row("File Size", format_file_size(output_size))
        results_table.add_row("Quantization", quantization.upper())
        results_table.add_row("Target Device", device_target.replace('_', ' ').title())
        
        if output_file.exists():
            results_table.add_row("Compression Ratio", f"{(model.stat().st_size / output_size):.1f}x")
        
        console.print(results_table)
        console.print(f"\n‚úÖ [bold green]Model export completed successfully![/bold green]")
    
    else:
        console.print("üß™ [yellow]Dry run completed - no files were exported[/yellow]")


@model.command()
@click.option('--model', '-m', required=True, type=Path, help='Model path to inspect')
@click.option('--show-config', is_flag=True, help='Show model configuration')
@click.option('--show-layers', is_flag=True, help='Show model architecture layers')
@click.option('--educational-check', is_flag=True, help='Check educational suitability')
def info(model: Path, show_config: bool, show_layers: bool, educational_check: bool):
    """Display detailed information about a model."""
    
    console.print("\nüîç [bold blue]Model Information[/bold blue]\n")
    
    if not model.exists():
        console.print(f"‚ùå [red]Model not found: {model}[/red]")
        raise click.Abort()
    
    try:
        model_info = _get_model_info(model)
        
        # Basic information table
        info_table = Table(title="Model Overview")
        info_table.add_column("Property", style="cyan")
        info_table.add_column("Value", style="white")
        
        info_table.add_row("Model Path", str(model))
        info_table.add_row("Model Type", model_info.get("type", "Unknown"))
        info_table.add_row("Format", model_info.get("format", "Unknown"))
        info_table.add_row("File Size", format_file_size(model_info.get("size", 0)))
        
        if "parameters" in model_info:
            info_table.add_row("Parameters", f"{model_info['parameters']:,}")
        
        if "architecture" in model_info:
            info_table.add_row("Architecture", model_info["architecture"])
        
        if "vocab_size" in model_info:
            info_table.add_row("Vocabulary Size", f"{model_info['vocab_size']:,}")
        
        console.print(info_table)
        
        # Device compatibility
        console.print("\nüñ•Ô∏è  [bold green]Device Compatibility[/bold green]\n")
        compatibility = _assess_device_compatibility(model_info)
        
        compat_table = Table()
        compat_table.add_column("Device", style="cyan")
        compat_table.add_column("Compatible", style="white")
        compat_table.add_column("Memory Required", style="yellow")
        
        for device, info in compatibility.items():
            status = "‚úì Yes" if info["compatible"] else "‚ùå No"
            memory = info.get("memory_required", "N/A")
            compat_table.add_row(device, status, memory)
        
        console.print(compat_table)
        
        # Educational suitability check
        if educational_check:
            console.print("\nüéì [bold green]Educational Suitability[/bold green]\n")
            
            edu_results = _check_educational_suitability(model, model_info)
            
            edu_table = Table()
            edu_table.add_column("Assessment", style="cyan")
            edu_table.add_column("Status", style="white")
            edu_table.add_column("Notes", style="yellow")
            
            for assessment, result in edu_results.items():
                status = "‚úì Pass" if result["passed"] else "‚ö†Ô∏è Warning"
                notes = result.get("notes", "")
                edu_table.add_row(assessment, status, notes)
            
            console.print(edu_table)
        
        # Model configuration
        if show_config and "config" in model_info:
            console.print("\n‚öôÔ∏è  [bold green]Model Configuration[/bold green]\n")
            config_text = json.dumps(model_info["config"], indent=2)
            console.print(Panel(config_text, title="config.json", border_style="blue"))
        
        # Architecture layers
        if show_layers and "layers" in model_info:
            console.print("\nüèóÔ∏è  [bold green]Model Architecture[/bold green]\n")
            
            layers_table = Table()
            layers_table.add_column("Layer", style="cyan")
            layers_table.add_column("Type", style="white")
            layers_table.add_column("Parameters", justify="right", style="yellow")
            
            for layer in model_info["layers"]:
                layers_table.add_row(
                    layer.get("name", "Unknown"),
                    layer.get("type", "Unknown"),
                    f"{layer.get('parameters', 0):,}"
                )
            
            console.print(layers_table)
    
    except Exception as e:
        console.print(f"‚ùå [red]Failed to analyze model: {e}[/red]")
        raise click.Abort()


@model.command()
@click.option('--model', '-m', required=True, type=Path, help='Model path to test')
@click.option('--prompt', '-p', default="Tell me a story about children helping their family.",
              help='Test prompt for content generation')
@click.option('--content-type', type=click.Choice([ct.value for ct in ContentType]),
              default='story', help='Type of content to generate')
@click.option('--age-group', type=click.Choice([ag.value for ag in AgeGroup]),
              default='primary_lower', help='Target age group')
@click.option('--max-length', default=100, help='Maximum generation length')
@click.option('--temperature', default=0.7, help='Generation temperature')
def test(model: Path, prompt: str, content_type: str, age_group: str, 
         max_length: int, temperature: float):
    """Test educational content generation with a model."""
    
    console.print("\nüß™ [bold blue]Testing Educational Content Generation[/bold blue]\n")
    
    if not model.exists():
        console.print(f"‚ùå [red]Model not found: {model}[/red]")
        raise click.Abort()
    
    # Convert string enums back to enum types
    content_type_enum = ContentType(content_type)
    age_group_enum = AgeGroup(age_group)
    
    # Display test configuration
    config_table = Table(title="Test Configuration")
    config_table.add_column("Setting", style="cyan")
    config_table.add_column("Value", style="white")
    
    config_table.add_row("Model", str(model))
    config_table.add_row("Prompt", prompt)
    config_table.add_row("Content Type", content_type.replace('_', ' ').title())
    config_table.add_row("Age Group", age_group.replace('_', ' ').title())
    config_table.add_row("Max Length", str(max_length))
    config_table.add_row("Temperature", str(temperature))
    
    console.print(config_table)
    console.print()
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        
        # Load model
        task1 = progress.add_task("Loading model...", total=1)
        try:
            # Placeholder for model loading - would need actual inference implementation
            progress.update(task1, completed=1)
            console.print("‚úì [green]Model loaded successfully[/green]")
        except Exception as e:
            progress.update(task1, completed=1)
            console.print(f"‚ùå [red]Failed to load model: {e}[/red]")
            raise click.Abort()
        
        # Generate content
        task2 = progress.add_task("Generating educational content...", total=1)
        try:
            # Placeholder for content generation - would need actual inference
            generated_text = _generate_educational_content(
                model, prompt, content_type_enum, age_group_enum, max_length, temperature
            )
            progress.update(task2, completed=1)
            console.print("‚úì [green]Content generated successfully[/green]")
        except Exception as e:
            progress.update(task2, completed=1)
            console.print(f"‚ùå [red]Content generation failed: {e}[/red]")
            raise click.Abort()
        
        # Validate generated content
        task3 = progress.add_task("Validating educational appropriateness...", total=1)
        try:
            validation_results = _validate_generated_content(generated_text, content_type_enum, age_group_enum)
            progress.update(task3, completed=1)
        except Exception as e:
            progress.update(task3, completed=1)
            console.print(f"‚ùå [red]Content validation failed: {e}[/red]")
            raise click.Abort()
    
    # Display results
    console.print("\nüìÑ [bold green]Generated Content[/bold green]\n")
    console.print(Panel(generated_text, title="Educational Content", border_style="green"))
    
    console.print("\nüìä [bold green]Validation Results[/bold green]\n")
    
    validation_table = Table()
    validation_table.add_column("Check", style="cyan")
    validation_table.add_column("Status", style="white")
    validation_table.add_column("Score", justify="right", style="yellow")
    
    for check, result in validation_results.items():
        status = "‚úì Pass" if result["passed"] else "‚ùå Fail"
        score = f"{result.get('score', 0):.2f}"
        validation_table.add_row(check, status, score)
    
    console.print(validation_table)
    
    overall_pass = all(result["passed"] for result in validation_results.values())
    if overall_pass:
        console.print("\n‚úÖ [bold green]Overall: CONTENT VALIDATION PASSED[/bold green]")
    else:
        console.print("\n‚ùå [bold red]Overall: CONTENT VALIDATION FAILED[/bold red]")


# Helper functions (placeholders for actual implementation)

def _get_model_info(model_path: Path) -> Dict[str, Any]:
    """Get detailed information about a model."""
    info = {
        "type": "Unknown",
        "format": "Unknown",
        "size": 0
    }
    
    if model_path.is_file():
        info["size"] = model_path.stat().st_size
        
        if model_path.suffix == '.gguf':
            info["format"] = "GGUF"
            info["type"] = "Quantized Language Model"
            # Placeholder - would need GGUF parser
            info["parameters"] = 7_000_000_000  # Example: 7B parameters
            info["architecture"] = "Transformer"
        elif model_path.suffix in ['.bin', '.pth']:
            info["format"] = "PyTorch"
            info["type"] = "PyTorch Model"
        elif model_path.suffix == '.safetensors':
            info["format"] = "SafeTensors"
            info["type"] = "SafeTensors Model"
    
    elif model_path.is_dir():
        # HuggingFace model directory
        info["format"] = "HuggingFace"
        info["type"] = "Transformers Model"
        info["size"] = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
        
        # Try to read config.json
        config_file = model_path / "config.json"
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                    info["config"] = config
                    info["architecture"] = config.get("model_type", "Unknown")
                    info["vocab_size"] = config.get("vocab_size", 0)
            except:
                pass
    
    return info


def _export_to_gguf(source_model: Path, output_file: Path, quantization: str, device: DeviceType):
    """Export model to GGUF format."""
    # Placeholder implementation - would need actual GGUF conversion
    console.print("[yellow]Note: GGUF export requires llama.cpp installation[/yellow]")
    console.print(f"[yellow]Would convert: {source_model} -> {output_file}[/yellow]")
    console.print(f"[yellow]Quantization: {quantization}[/yellow]")
    console.print(f"[yellow]Target device: {device.value}[/yellow]")
    
    # Create placeholder output file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text("# Placeholder GGUF file\n# Actual implementation would use llama.cpp\n")


def _validate_educational_generation(model_file: Path) -> Dict[str, Any]:
    """Validate model's educational content generation capability."""
    # Placeholder implementation
    return {
        "passed": True,
        "issues": []
    }


def _assess_device_compatibility(model_info: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """Assess model compatibility with different devices."""
    size_mb = model_info.get("size", 0) / (1024 * 1024)
    
    compatibility = {
        "Raspberry Pi": {
            "compatible": size_mb < 2000,  # 2GB limit
            "memory_required": f"{size_mb * 1.5:.0f} MB"
        },
        "Desktop CPU": {
            "compatible": size_mb < 8000,  # 8GB limit
            "memory_required": f"{size_mb * 2:.0f} MB"
        },
        "GPU (8GB)": {
            "compatible": size_mb < 6000,  # Leave headroom
            "memory_required": f"{size_mb:.0f} MB"
        },
        "GPU (16GB+)": {
            "compatible": True,
            "memory_required": f"{size_mb:.0f} MB"
        }
    }
    
    return compatibility


def _check_educational_suitability(model_path: Path, model_info: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """Check if model is suitable for educational use."""
    # Placeholder implementation
    return {
        "Content Safety": {
            "passed": True,
            "notes": "Model should generate age-appropriate content"
        },
        "Language Quality": {
            "passed": True,
            "notes": "Model should generate grammatically correct text"
        },
        "Cultural Sensitivity": {
            "passed": True,
            "notes": "Model should respect Toaripi cultural context"
        },
        "Educational Value": {
            "passed": True,
            "notes": "Model should support educational objectives"
        }
    }


def _generate_educational_content(model_path: Path, prompt: str, content_type: ContentType, 
                                age_group: AgeGroup, max_length: int, temperature: float) -> str:
    """Generate educational content using the model."""
    # Placeholder implementation - would need actual inference engine
    return f"""Once upon a time in a Toaripi village, there were children who loved to help their families. 
Every morning, they would wake up early and ask their parents, "How can we help today?" 

The children learned that helping their family was one of the most important things they could do. 
They helped with fishing, cooking, and taking care of their younger brothers and sisters.

The whole village was proud of these helpful children, and they grew up to be kind and responsible adults.

The end."""


def _validate_generated_content(text: str, content_type: ContentType, age_group: AgeGroup) -> Dict[str, Dict[str, Any]]:
    """Validate generated content for educational appropriateness."""
    from ..data.preprocessing import TextCleaner
    
    cleaner = TextCleaner()
    
    return {
        "Content Appropriateness": {
            "passed": cleaner.is_appropriate_content(text),
            "score": 0.95
        },
        "Educational Value": {
            "passed": cleaner.calculate_educational_score(text) >= 0.7,
            "score": cleaner.calculate_educational_score(text)
        },
        "Age Appropriateness": {
            "passed": True,  # Placeholder logic
            "score": 0.90
        },
        "Cultural Sensitivity": {
            "passed": True,  # Placeholder logic
            "score": 0.88
        }
    }


if __name__ == '__main__':
    model()